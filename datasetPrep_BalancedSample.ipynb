{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd  # Library for applying functions to dataframes in parallel \n",
    "from dask.diagnostics import ProgressBar  # To check progress of dask computation(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original file too large to upload to github. Dataset processed locally.\n",
    "# Dataset location: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents\n",
    "\n",
    "data = pd.read_csv('US_Accidents_March23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Count:\n",
      " 2    6156981\n",
      "3    1299337\n",
      "4     204710\n",
      "1      67366\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Class Distribution Percentage:\n",
      " 2    79.667017\n",
      "3    16.812510\n",
      "4     2.648804\n",
      "1     0.871669\n",
      "Name: Severity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = data['Severity'].value_counts()\n",
    "\n",
    "# To get a percentage distribution, you can use:\n",
    "class_distribution_percent = data['Severity'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Printing the results\n",
    "print(\"Class Distribution Count:\\n\", class_distribution)\n",
    "print(\"\\nClass Distribution Percentage:\\n\", class_distribution_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat',\n",
       "       'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description',\n",
       "       'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone',\n",
       "       'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)',\n",
       "       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction',\n",
       "       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity',\n",
       "       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
       "       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n",
       "       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
       "       'Astronomical_Twilight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7487604, 46)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to remove datapoints with missing values in the following columns:\n",
    "columns_to_check = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Sunrise_Sunset']\n",
    "\n",
    "# Drop rows with missing values in the specified columns\n",
    "data = data.dropna(subset=columns_to_check)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to remove data points where date/time value is incorrect\n",
    "Initially, tried the approach below but was taking much too long\n",
    "\n",
    "# Function to check if a date string is in the correct format\n",
    "def is_valid_date(date_str):\n",
    "    try:\n",
    "        pd.to_datetime(date_str)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Filter out rows with invalid 'Start_Time' or 'End_Time'\n",
    "valid_start_time = data['Start_Time'].apply(is_valid_date)\n",
    "valid_end_time = data['End_Time'].apply(is_valid_date)\n",
    "data_clean = data[valid_start_time & valid_end_time]\n",
    "\n",
    "Will instead use the Dask library to perform this task in parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried this approach as well. Taking much too long.\n",
    "\n",
    "dask_data = dd.from_pandas(data, npartitions=4)\n",
    "\n",
    "# Filter out rows with invalid 'Start_Time' or 'End_Time'\n",
    "valid_start_time = dask_data['Start_Time'].map_partitions(lambda x: x.apply(is_valid_date), meta=bool)\n",
    "valid_end_time = dask_data['End_Time'].map_partitions(lambda x: x.apply(is_valid_date), meta=bool)\n",
    "\n",
    "dask_data_clean = dask_data[valid_start_time & valid_end_time]\n",
    "\n",
    "# Dask performs computations in 'lazy' manner, must call compute() to trigger actual computation\n",
    "# The compute function returns a pandas dataframe\n",
    "\n",
    "# Wrapping computation in a ProgressBar context manager\n",
    "with ProgressBar():\n",
    "    dask_data_clean = dask_data_clean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a date string is in the correct format\n",
    "def is_valid_date(date_str):\n",
    "    try:\n",
    "        pd.to_datetime(date_str)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will try filtering at the point of subset creation. This should be much more efficient than iterating over the entire original dataset\n",
    "\n",
    "def sample_and_filter(df, class_value, sample_size, date_columns):\n",
    "    # Sample the DataFrame\n",
    "    sample_df = df[df['Severity'] == class_value].sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Filter out invalid dates\n",
    "    for col in date_columns:\n",
    "        sample_df = sample_df[sample_df[col].apply(is_valid_date)]\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Columns to check for valid dates\n",
    "date_columns = ['Start_Time', 'End_Time']\n",
    "\n",
    "# Sample size for each class\n",
    "sample_size = 20000\n",
    "\n",
    "# Creating the samples\n",
    "class_1_sample = sample_and_filter(data, 1, sample_size, date_columns)\n",
    "class_2_sample = sample_and_filter(data, 2, sample_size, date_columns)\n",
    "class_3_sample = sample_and_filter(data, 3, sample_size, date_columns)\n",
    "class_4_sample = sample_and_filter(data, 4, sample_size, date_columns)\n",
    "\n",
    "# Combine the samples into a balanced dataset\n",
    "balanced_data = pd.concat([class_1_sample, class_2_sample, class_3_sample, class_4_sample])\n",
    "\n",
    "# Shuffle the dataset (optional but recommended)\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Count:\n",
      " 3    20000\n",
      "4    20000\n",
      "1    20000\n",
      "2    20000\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Class Distribution Percentage:\n",
      " 3    25.0\n",
      "4    25.0\n",
      "1    25.0\n",
      "2    25.0\n",
      "Name: Severity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Testing our approach \n",
    "class_distribution = balanced_data['Severity'].value_counts()\n",
    "\n",
    "# To get a percentage distribution, you can use:\n",
    "class_distribution_percent = balanced_data['Severity'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Printing the results\n",
    "print(\"Class Distribution Count:\\n\", class_distribution)\n",
    "print(\"\\nClass Distribution Percentage:\\n\", class_distribution_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balanced dataset to a CSV file\n",
    "balanced_data.to_csv('US_Accidents_Balanced_Sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
